apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
  namespace: kube-infra
spec:
  chart:
    spec:
      chart: loki
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  interval: 10m0s
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    crds: CreateReplace
    remediation:
      retries: 1
      strategy: uninstall
  dependsOn:
    - name: kube-prometheus-stack
  values:
    serviceMonitor:
      enabled: true
      additionalLabels:
        prometheus: default
      prometheusRule:
        enabled: true
        rules:
          - alert: LokiProcessTooManyRestarts
            expr: changes(process_start_time_seconds{job=~"loki"}[15m]) > 2
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Loki process too many restarts (instance {{ $labels.instance }})
              description: "A loki process had too many restarts (target {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: LokiRequestErrors
            expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
            for: 15m
            labels:
              severity: critical
            annotations:
              summary: Loki request errors (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} and {{ $labels.route }} are experiencing errors\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: LokiRequestPanic
            expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Loki request panic (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: LokiRequestLatency
            expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Loki request latency (instance {{ $labels.instance }})
              description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  postRenderers:
    - kustomize:
        patchesStrategicMerge:
          - apiVersion: monitoring.coreos.com/v1
            kind: ServiceMonitor
            metadata:
              name: loki
              labels:
                release: kube-prometheus-stack
          - apiVersion: monitoring.coreos.com/v1
            kind: PrometheusRule
            metadata:
              name: loki
              labels:
                release: kube-prometheus-stack
